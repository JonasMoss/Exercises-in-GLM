\section*{Additional Exercise 3}

\subsection*{(a)}
The model matrix $X$ as defined as the matrix satisfying 
\[
X\beta=\sum_{i=0}^{I}\beta_{i}X_{ni},
\]
hence $X=[1_{n},x-\overline{x}1_{n}]$ as claimed. Its rank is $2$,
since either among $x-\overline{x}1_{n}$ is different from the others
or $x-\overline{x}1_{n}=0$.

\subsection*{(b)}
We know that $P_{X}=X(X^{T}X)^{-1}X^{T}$. Let $Z=x-\overline{x}1_{n}$
and observe that 
\[
X^{T}X=\left[\begin{array}{cc}
1_{n}^{T}1_{n} & 1_{n}^{T}Z\\
1_{n}^{T}Z & Z^{T}Z
\end{array}\right].
\]
Now we use some facts to simplify $X^{T}X$,
\begin{eqnarray*}
1_{n}^{T}Z & = & \sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}\overline{x}=0,\\
1_{n}^{T}1_{n} & = & n,\\
Z^{T}Z & = & \sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{eqnarray*}
Plug these values into $X^{T}X$ and obtain its inverse matrix
\[
(X^{T}X)^{-1}=\left[\begin{array}{cc}
n^{-1} & 0\\
0 & M^{-1}
\end{array}\right].
\]
Moreover, since 
\[
\left[\begin{array}{cc}
C & D\end{array}\right]\left[\begin{array}{cc}
A & 0\\
0 & B
\end{array}\right]\left[\begin{array}{cc}
C & D\end{array}\right]^{T}=\left[\begin{array}{c}
CAC^{T}\\
BAB^{T}
\end{array}\right],
\]
we obtain
\[
P_{X}=X(X^{T}X)^{-1}X^{T}=n1_{n}1_{n}^{T}+M^{-1}(x-\overline{x})(x-\overline{x})^{T},
\]
as claimed.

\subsection*{(c)}
The fitted values are defined as
\[
\hat{\mu}=P_{X}y,
\]
and can be described using the estimated parameters in the following
way. Since 
\[
P_{X}y=n1_{n}1_{n}^{T}y+M^{-1}(x-\overline{x})(x-\overline{x})^{T}y,
\]
we get that
\[
n^{-1}1_{n}1_{n}^{T}y=1_{n}\overline{y}=1_{n}\hat{\beta_{0}}.
\]
Here $\overline{y}=\hat{\beta}_{0}$ since $\hat{\beta}_{0}=\overline{y}-\hat{\beta}_{1}\overline{(x-\overline{x})}$,
but $\overline{(x-\overline{x})}=0$. 
Moreover,
\begin{eqnarray*}
M^{-1}(x-\overline{x})(x-\overline{x})^{T}y & = & (x-\overline{x})\frac{(x-\overline{x})^{T}(y-1_{n}\overline{y})}{M}+(x-\overline{x})\frac{(x-\overline{x})^{T}1_{n}\overline{y}}{M}.\\
 & = & (x-\overline{x})\hat{\beta}_{1}+0,\\
 & = & (x-\overline{x})\hat{\beta}_{1}.
\end{eqnarray*}
And we can conclude that
\[
\hat{\mu}=1_{n}\hat{\beta_{0}}+(x-\overline{x})\hat{\beta}_{1}.
\]

\subsection*{(d)}
We employ the decomposition
\[
Y^{T}Y=Y^{T}P_{0}Y+Y^{T}(P_{X}-P_{0})Y+Y^{T}(I-P_{X})Y.
\]
Now we make use of the following facts to simplify the expression.
\begin{eqnarray*}
P_{0}Y & = & 1_{n}\overline{y},\\
(P_{X}-P_{0})Y & = & 1_{n}(\hat{\mu}-\overline{y}),\\
(I-P_{X})Y & = & Y-1_{n}\hat{\mu}.
\end{eqnarray*}
Since projection matrices satisfy $P^{2}=P$ and $P^{T}=P$, we obtain
\begin{eqnarray*}
Y^{T}P_{0}Y & = & (P_{0}Y)^{T}P_{0}Y=n\overline{y}^{2},\\
(P_{X}-P_{0})Y & = & [(P_{X}-P_{0})Y]^{T}(P_{X}-P_{0})Y=\sum_{i=1}^{n}(\hat{\mu_{i}}-\overline{y})^{2},\\
(I-P_{X})Y & = & [(I-P_{X})Y]^{T}(I-P_{X})Y=\sum_{i=1}^{n}(y_{i}-\hat{\mu})^{2}.
\end{eqnarray*}

\subsection*{(e)}
They are independently $\chi^{2}$-distributed since the projections
sum up to $I$. The rank of $P_{X}-P_{0}$ is $1$ since the rank
of $P_{X}$ is $2$, and the rank of $P_{0}$ is $1$. Its non-centrality
parameter is $\sigma^{2}\mu(P_{X}-P_{0})\mu$.
Since
\[
\mu=\beta_{0}1_{n}+\beta_{1}(x-\overline{x})
\]
and
\[
P_{X}-P_{0}=M^{-1}(x-\overline{x}1_{n})^{T}(x-\overline{x}1_{n})
\]
and
\begin{eqnarray*}
(x-\overline{x}1_{n})^{T}\mu & = & (x-\overline{x}1_{n})^{T}(\beta_{0}1_{n}+\beta_{1}(x-\overline{x})),\\
 & = & \beta_{0}(x^{T}1_{n}-\overline{x}1_{n}^{T}1_{n})+\beta_{1}M,\\
 & = & \beta_{1}M,
\end{eqnarray*}
we obtain the non-centrality parameter 
\[
M^{-1}\sigma^{-2}\mu^{T}(P_{X}-P_{0})\mu=\sigma^{-2}M\beta_{1}^{-2}.
\]
As for $Y^{T}(I-P_{X})Y\sigma^{-2}$, the rank is $n-2$ while $\mu^{T}(\mu-P_{X}\mu)=0$,
hence its non-centrality parameter is $0$.

\subsection*{(f)}
We should use 
\[
\frac{Y^{T}(P_{X}-P_{0})Y/1}{Y^{T}(I-P_{X})Y/(n-2)}\sim F_{1,n-2,\sigma^{-2}M\beta_{1}^{2}}.
\]
Under $H_{0},$ the non-centrality is $0$.
